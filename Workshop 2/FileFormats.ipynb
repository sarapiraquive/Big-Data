{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8188beb12e205e",
   "metadata": {},
   "source": [
    "# Universidad de la Sabana\n",
    "## Big Data Tools\n",
    "### Coterminal - Ingeniería Informática\n",
    "### Prof. Hugo Franco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T11:15:02.370189Z",
     "start_time": "2025-08-09T11:15:02.367863Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e6c9b5204ffc6",
   "metadata": {},
   "source": [
    "- We'll define the paths for the files selected for the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45af2fde5722778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'result_retrieve_left-and-right_x_50_2016_modified.csv'\n",
    "parquet_path = 'result_retrieve_left-and-right_x_50_2016_modified.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd58310200c452f",
   "metadata": {},
   "source": [
    "- Now, we'll load the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b642ae2c978ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the modified files ---\n",
    "print(\"Loading modified files into new DataFrames...\")\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "df_parquet = pd.read_parquet(parquet_path)\n",
    "\n",
    "print(\"Modified files loaded successfully. Here is a preview of the data:\")\n",
    "display(df_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195aaaa0f68e899f",
   "metadata": {},
   "source": [
    "- Let's compare the disk size and the shape (rows, columns) of the two file formats. You'll notice that Parquet is significantly more efficient for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db34b6aad778a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare file size and DataFrame shape ---\n",
    "\n",
    "# Get file sizes\n",
    "csv_size_bytes = os.path.getsize(csv_path)\n",
    "parquet_size_bytes = os.path.getsize(parquet_path)\n",
    "\n",
    "# Get DataFrame shapes\n",
    "csv_rows, csv_cols = df_csv.shape\n",
    "parquet_rows, parquet_cols = df_parquet.shape\n",
    "\n",
    "# Print comparison\n",
    "print(\"--- File and DataFrame Comparison ---\")\n",
    "print(\"\\nCSV File:\")\n",
    "print(f\"  - File Path: {csv_path}\")\n",
    "print(f\"  - Size on disk: {csv_size_bytes / 1024:.2f} KB\")\n",
    "print(f\"  - Shape: {csv_rows} rows, {csv_cols} columns\")\n",
    "\n",
    "print(\"\\nParquet File:\")\n",
    "print(f\"  - File Path: {parquet_path}\")\n",
    "print(f\"  - Size on disk: {parquet_size_bytes / 1024:.2f} KB\")\n",
    "print(f\"  - Shape: {parquet_rows} rows, {parquet_cols} columns\")\n",
    "\n",
    "# Highlight the size difference\n",
    "size_difference = (csv_size_bytes - parquet_size_bytes) / csv_size_bytes * 100\n",
    "print(f\"\\nNote: The Parquet file is {size_difference:.2f}% smaller than the CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f82bffe29c0f05",
   "metadata": {},
   "source": [
    "- The .describe() method provides a powerful statistical summary of the data. Using include='all' gives us statistics for both numerical and text-based columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea515fb127a6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Obtain a statistical description of the DataFrame ---\n",
    "# (We only need to run this on one DataFrame, as they contain identical data)\n",
    "\n",
    "print(\"--- Statistical Description ---\")\n",
    "df_parquet.info()\n",
    "display(df_parquet.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3212268b29422cd",
   "metadata": {},
   "source": [
    "This is the core analysis step. We group the data by the specified categories and calculate the average value_x, value_y, and value_z for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551187886f1f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create average values for x, y, and z columns ---\n",
    "\n",
    "# Define the columns to group by and the columns to aggregate\n",
    "grouping_cols = ['fact_id', 'side', 'joint', 'variable']\n",
    "value_cols = ['value_x', 'value_y', 'value_z']\n",
    "\n",
    "print(f\"Grouping by {grouping_cols} and calculating the mean of {value_cols}...\")\n",
    "\n",
    "# Perform the groupby and aggregation.\n",
    "# .reset_index() converts the grouped columns back into regular columns.\n",
    "df_agg = df_parquet.groupby(grouping_cols)[value_cols].mean().reset_index()\n",
    "\n",
    "# Rename columns for clarity in the database\n",
    "df_agg.rename(columns={\n",
    "    'value_x': 'avg_x',\n",
    "    'value_y': 'avg_y',\n",
    "    'value_z': 'avg_z'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"\\nPreview of the final data to be loaded:\")\n",
    "display(df_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7ec0d9f9a0021",
   "metadata": {},
   "source": [
    "Handling Missing Data:\n",
    "•The value_x, value_y, and value_z columns have some missing entries. First, calculate and print the total number of missing values for each of these three columns.\n",
    "•Create a new, cleaned DataFrame by dropping all rows that have missing values in any of those three columns (value_x, value_y, or value_z).\n",
    "•Verify your work by checking for missing values again in the new DataFrame.3.Data Filtering and Subsetting:•From your cleaned DataFrame (from Question 2), remove the columns sd_x, sd_y, sd_z, md_x, md_y, and md_z, as they are not needed for this analysis.\n",
    "•Create a new DataFrame that contains only the data for the 'Hip' joint. How many rows remain in this new 'Hip' DataFrame?\n",
    "\n",
    "File Format Comparison:\n",
    "•Take the final 'Hip' DataFrame from Question 3 and save it to two new files: hip_data.csv and hip_data.parquet.\n",
    "•Using Python's os library, get the size of each file on disk.\n",
    "•Calculate and print the percentage difference in size, showing how much smaller the Parquet file is compared to the CSV.\n",
    "\n",
    "Advanced Pandas Aggregation:\n",
    "•Using the full cleaned DataFrame (from Question 2, before filtering for the 'Hip' joint), group the data by side and variable.\n",
    "•For each group, calculate the standard deviation (std) of value_x, value_y, and value_z.\n",
    "•Display the resulting aggregated DataFrame. Which variable shows the highest standard deviation for value_x on the 'L' (Left) side?6.Finding a Maximum Value:\n",
    "•Using the full cleaned DataFrame, find the fact_id that corresponds to the single highest value_y measurement recorded in the entire dataset. (Hint: You might find the .idxmax() method useful)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11822379",
   "metadata": {},
   "source": [
    "### Calculate and print the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14613e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "value_x    1584\n",
      "value_y    1584\n",
      "value_z    1584\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_counts = df_parquet[['value_x', 'value_y', 'value_z']].isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bc5df",
   "metadata": {},
   "source": [
    "### Create a new cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_parquet.dropna(subset=['value_x', 'value_y', 'value_z'])\n",
    "\n",
    "print(f\"\\nRows before cleaning: {df_parquet.shape[0]}\")\n",
    "print(f\"Rows after cleaning: {df_clean.shape[0]}\")\n",
    "\n",
    "missing_counts_clean = df_clean[['value_x', 'value_y', 'value_z']].isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_counts_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb985045",
   "metadata": {},
   "source": [
    "### Drop certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "273c8c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after dropping unnecessary ones:\n",
      "Index(['fact_id', 'year', 'subject_id', 'date', 'otp', 'trial', 'group',\n",
      "       'marker', 'side', 'joint', 'variable', 'units', 'protocol', 'value_x',\n",
      "       'value_y', 'value_z'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.drop(columns=['sd_x', 'sd_y', 'sd_z', 'md_x', 'md_y', 'md_z'])\n",
    "print(\"Columns after dropping unnecessary ones:\")\n",
    "print(df_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da38815",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
